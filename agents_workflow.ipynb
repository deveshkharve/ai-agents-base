{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL5B7oLB4mRV",
        "outputId": "956b71a7-8578-4e66-a7f3-114b63fb16fb"
      },
      "outputs": [],
      "source": [
        "!pip install -U langgraph langchain langchain_openai langchain_experimental langsmith pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, List, Union, Tuple\n",
        "from langchain_core.agents import AgentAction, AgentFinish\n",
        "import operator\n",
        "\n",
        "\n",
        "class JarvisState(TypedDict):\n",
        "    input: str\n",
        "    agent_out: Union[AgentAction, AgentFinish, None]\n",
        "    protocol: dict\n",
        "    intermediate_steps: Annotated[List[Tuple[AgentAction, str]], operator.add]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from pydantic import BaseModel\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from enum import Enum, auto\n",
        "\n",
        "class CoffeeQuery(BaseModel):\n",
        "    type: str\n",
        "    serving_size: str\n",
        "\n",
        "\n",
        "@tool(\"get_coffee\")\n",
        "def make_coffee_tool(query: CoffeeQuery):\n",
        "    \"\"\"Prepares coffee for of the given type and serving_size.\n",
        "    `type` can be any one of (AMERICANO, CAPPUCCINO, MOCHA)\n",
        "    `serving_size` can be any of (REGULAR, LARGE, GRANDE)\n",
        "    Cannot be used to for any other topics.\"\"\"\n",
        "\n",
        "    return f'Here is your {query.type} in you favorite {query.serving_size} cup.'\n",
        "\n",
        "@tool(\"get_chat_response\")\n",
        "def role_play_chat_tool(query: str):\n",
        "    \"\"\"Generates a chat response on any given topic. query must be provided\n",
        "    in natural language and be verbose.\n",
        "    \"\"\"\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "    prompt = f\"You are Ultron. You are an expert in all topics. You respond to all queies in third-person way.\\n\\n\\nquery: {query}\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "\n",
        "@tool(\"send_email\")\n",
        "def send_email_tool(\n",
        "    recipient: str,\n",
        "    subject: str,\n",
        "    body: str\n",
        "):\n",
        "    \"\"\"Sends email to the given recipient with a valid subject and email body\"\"\"\n",
        "\n",
        "    print('sending email.....', recipient, subject, body)\n",
        "    return f\"Email:\\n\\n\\n To: {recipient}\\n\\nSubject: {subject}\\n\\nBody: {body}\"\n",
        "\n",
        "\n",
        "@tool(\"special_protocols\")\n",
        "def special_protocols_tool(\n",
        "    protocol_name: str\n",
        "):\n",
        "    \"\"\"Checks and Runs the given protocol for the selected target. Must be used only while triggering a protocol\n",
        "    \"\"\"\n",
        "\n",
        "    # protocol_name can be one of (theta, house-party, training-wheels)\n",
        "    if protocol_name.lower() not in ['theta', 'house-party', 'training-wheels']:\n",
        "        return { 'message': f'Cannot find the {protocol_name} protocol, Tony!', 'registered': False, 'protocol_name': protocol_name }\n",
        "\n",
        "    return { 'message': f'Okay, triggering {protocol_name}', 'registered': True, 'protocol_name': protocol_name }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.agents import create_openai_tools_agent\n",
        "from langchain import hub\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "user_name = 'Tony Stark'\n",
        "prompt.messages[0].prompt.template = f'You are a helpful assistant named Jarvis, you answer to {user_name}'\n",
        "\n",
        "query_agent_runnable = create_openai_tools_agent(\n",
        "    llm=llm,\n",
        "    tools=[make_coffee_tool, role_play_chat_tool, send_email_tool, special_protocols_tool],\n",
        "    prompt=prompt\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = {\n",
        "    \"input\": \"Hey jarvis, send email to im.hulk@avengers.com saying wassup Bruce, hope alls well, want to discuss about Vibranium, meet me\",\n",
        "    \"intermediate_steps\": []\n",
        "}\n",
        "agent_out = query_agent_runnable.invoke(inputs)\n",
        "agent_out\n",
        "\n",
        "inputs = {\n",
        "    \"input\": \"Jarvis, tell me about who is Zeus?\",\n",
        "    \"intermediate_steps\": []\n",
        "}\n",
        "agent_out = query_agent_runnable.invoke(inputs)\n",
        "agent_out\n",
        "\n",
        "inputs = {\n",
        "    \"input\": \"where is Delhi?\",\n",
        "    \"intermediate_steps\": []\n",
        "}\n",
        "agent_out = query_agent_runnable.invoke(inputs)\n",
        "agent_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def capture_task(state: JarvisState):\n",
        "    print(\"capture_task\")\n",
        "    agent_out = query_agent_runnable.invoke(state)\n",
        "    \n",
        "    return {\"agent_out\": agent_out}\n",
        "\n",
        "def check_task_type_router(state: JarvisState):\n",
        "    print(\"check_task_type_router\")\n",
        "    action = state[\"agent_out\"]\n",
        "    \n",
        "    if \"tool_calls\" in action[-1].message_log[-1].additional_kwargs:\n",
        "        tool_call = action[-1].message_log[-1].additional_kwargs[\"tool_calls\"][-1]\n",
        "        function_name = tool_call[\"function\"][\"name\"] \n",
        "\n",
        "        if function_name == \"special_protocols\":\n",
        "            return \"is_special_protocols\"\n",
        "    \n",
        "    return \"is_routine_task\"\n",
        "\n",
        "\n",
        "def execute_task(state: JarvisState):\n",
        "    print(\"execute_task\")\n",
        "    action = state[\"agent_out\"]\n",
        "    tool_call = action[-1].message_log[-1].additional_kwargs[\"tool_calls\"][-1]\n",
        "    function_name = tool_call[\"function\"][\"name\"] \n",
        "    function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
        "\n",
        "    if function_name == \"get_coffee\":\n",
        "        return { \"agent_out\": make_coffee_tool(function_args)}\n",
        "\n",
        "    if function_name == \"get_chat_response\":\n",
        "        return { \"agent_out\": role_play_chat_tool(function_args) }\n",
        "\n",
        "    if function_name == \"send_email\":\n",
        "        return { \"agent_out\": send_email_tool(function_args) }\n",
        "    \n",
        "\n",
        "# answer_formatter_llm = formatter_llm.bind_tools([response_formatter_tool])\n",
        "\n",
        "def check_for_protocol(state: JarvisState):\n",
        "    print('check_for_protocol')\n",
        "\n",
        "    action = state[\"agent_out\"]\n",
        "    tool_call = action[-1].message_log[-1].additional_kwargs[\"tool_calls\"][-1]\n",
        "    function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
        "    \n",
        "    protocol_registered = special_protocols_tool(function_args)\n",
        "\n",
        "    return { 'agent_out': protocol_registered['message'], 'protocol': protocol_registered }\n",
        "\n",
        "\n",
        "def protocol_router(state: JarvisState):\n",
        "    print('protocol_router')\n",
        "    protocol_data = state['protocol']\n",
        "\n",
        "    if protocol_data['registered']:\n",
        "        return 'registered'\n",
        "\n",
        "    return 'not_registered'\n",
        "\n",
        "import random\n",
        "def reject_protocol(state: JarvisState):\n",
        "    print('reject_protocol')\n",
        "    protocol_data = state['protocol']\n",
        "\n",
        "    print('protocol_data', protocol_data, protocol_data['protocol_name'])\n",
        "\n",
        "    return { 'agent_out': f'Hey Sorry Tony, you need to register the protocol {protocol_data['protocol_name']} first.' }\n",
        "\n",
        "import random\n",
        "def execute_protocol(state: JarvisState):\n",
        "    print('execute_protocol')\n",
        "    protocol_data = state['protocol']\n",
        "    random_number = random.randint(1, 10)\n",
        "    if random_number%2 == 0:\n",
        "        return { agent_out: f'Failed to execute protocol {protocol_data['protocol_name']}' }\n",
        "    \n",
        "    return { 'agent_out': f'Executing protocol {protocol_data['protocol_name']}' }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END, START\n",
        "\n",
        "graph = StateGraph(JarvisState)\n",
        "\n",
        "graph.add_node(\"capture_task\", capture_task)\n",
        "graph.add_node(\"execute_task\", execute_task)\n",
        "graph.add_node(\"check_for_protocol\", check_for_protocol)\n",
        "graph.add_node(\"execute_protocol\", execute_protocol)\n",
        "graph.add_node(\"reject_protocol\", reject_protocol)\n",
        "\n",
        "\n",
        "graph.set_entry_point(\"capture_task\")\n",
        "\n",
        "# conditional edges are controlled by our router\n",
        "graph.add_conditional_edges(\n",
        "    source=\"capture_task\",  # where in graph to start\n",
        "    path=check_task_type_router,  # function to determine which node is called\n",
        "    path_map={\n",
        "        'is_special_protocols': \"check_for_protocol\",\n",
        "        'is_routine_task': \"execute_task\"\n",
        "    }\n",
        ")\n",
        "graph.add_edge(\"execute_task\", END)\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    source=\"check_for_protocol\",  # where in graph to start\n",
        "    path=protocol_router,  # function to determine which node is called\n",
        "    path_map={\n",
        "        'registered': 'execute_protocol',\n",
        "        'not_registered': 'reject_protocol'\n",
        "    }\n",
        ")\n",
        "\n",
        "graph.add_edge(\"execute_protocol\", END)\n",
        "graph.add_edge(\"reject_protocol\", END)\n",
        "\n",
        "\n",
        "\n",
        "runnable = graph.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(runnable.get_graph(xray=True).draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "out = runnable.invoke({\n",
        "    \"input\": \"jarvis please make me a regular cappuchina?\",\n",
        "    \"chat_history\": []\n",
        "})\n",
        "print(out[\"agent_out\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "out = runnable.invoke({\n",
        "    \"input\": \"Tell me about Anubis\",\n",
        "    \"chat_history\": []\n",
        "})\n",
        "print(out[\"agent_out\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
